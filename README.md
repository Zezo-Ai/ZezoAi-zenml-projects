# Creditâ€‘Scoring EU AI Act Demo

> A ZenMLâ€‘powered endâ€‘toâ€‘end creditâ€‘scoring workflow that automatically generates the technical evidence required by the EU AI Act.

## ðŸš€ Project Overview

The project implements three main pipelines:

1. **Feature Engineering**: Handles data governance and preprocessing (Articles 10, 12, 15)
   â€“ `ingest â†’ data_splitter â†’ data_preprocessor â†’ generate_compliance_metadata`

2. **Training Pipeline**: Implements model training, evaluation, and risk assessment (Articles 9, 11, 15)  
   â€“ `train_model â†’ evaluate_model â†’ risk_assessment`

3. **Deployment Pipeline**: Manages human oversight, deployment, and monitoring (Articles 14, 17, 18)
   â€“ `approve_deployment â†’ modal_deployment â†’ post_market_monitoring â†’ post_run_annex`

Each run automatically versions its inputs, logs hashes & metrics, and generates a complete Annexâ€¯IV draft.

## Architecture

![End-to-End Architecture](docs/e2e.png)

## Project Structure

```bash
credit_scoring_ai_act/
â”œâ”€â”€ app/ # Modal deployment app
â”‚   â”œâ”€â”€ modal_deployment.py # Modal deployment script
â”‚   â””â”€â”€ schemas.py # Pydantic models
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ pipelines/
â”‚   â”‚   â”œâ”€â”€ feature_engineering.py # Feature engineering pipeline
â”‚   â”‚   â”œâ”€â”€ training.py # Model training pipeline
â”‚   â”‚   â””â”€â”€ deployment.py # Deployment pipeline
â”‚   â”œâ”€â”€ steps/
â”‚   â”‚   â”œâ”€â”€ ingest.py # Load CSV â†’ log SHAâ€‘256, WhyLogs profile
â”‚   â”‚   â”œâ”€â”€ data_preprocessor.py # Basic feature engineering
â”‚   â”‚   â”œâ”€â”€ data_splitter.py # Split dataset into train/test
â”‚   â”‚   â”œâ”€â”€ generate_compliance_metadata.py # Generate compliance metadata
â”‚   â”‚   â”œâ”€â”€ train.py # XGBoost / sklearn model
â”‚   â”‚   â”œâ”€â”€ evaluate.py # Standard + Fairness metrics
â”‚   â”‚   â”œâ”€â”€ approve.py # Humanâ€‘inâ€‘loop gate (approve_deployment step)
â”‚   â”‚   â”œâ”€â”€ post_market_monitoring.py # Postâ€‘market monitoring
â”‚   â”‚   â”œâ”€â”€ generate_sbom.py # Generate SBOM
â”‚   â”‚   â”œâ”€â”€ post_run_annex.py # Generate Annex IV documentation
â”‚   â”‚   â”œâ”€â”€ risk_assessment.py # Risk assessment
â”‚   â”‚   â””â”€â”€ deploy.py # Push to Modal / local FastAPI
â”‚   â”œâ”€â”€ utils/ # Shared utilities
â”‚   â”‚   â”œâ”€â”€ modal_utils.py # Modal Volume operations
â”‚   â”‚   â”œâ”€â”€ preprocess.py # Custom sklearn transformers
â”‚   â”‚   â”œâ”€â”€ eval.py # Evaluation utils
â”‚   â”‚   â”œâ”€â”€ incidents.py # Incident reporting system
â”‚   â”‚   â”œâ”€â”€ visualizations.py # Visualization utils
â”‚   â”‚   â””â”€â”€ model_definition.py # ZenML model definition
â”‚   â”‚
â”‚   â”œâ”€â”€ configs/ # Configuration files
â”‚   â””â”€â”€ constants.py # Centralized configuration constants
â”‚
â”œâ”€â”€ docs/
â”‚   â”œâ”€â”€ risk/ # Autoâ€‘generated annex iv reports after deployment
â”‚   â”œâ”€â”€ releases/ # Manual compliance inputs organized by run ID
â”‚   â”‚   â”œâ”€â”€ 07c7fb5f-34d7-48f8-af4f-2bd87e58a73a/  # Example run ID
â”‚   â”‚   â”‚   â”œâ”€â”€ annex_iv_cs_deployment.md  # Annex IV deployment documentation
â”‚   â”‚   â”‚   â”œâ”€â”€ evaluation_results.yaml    # Model evaluation metrics
â”‚   â”‚   â”‚   â”œâ”€â”€ git_info.md                # Git repository information
â”‚   â”‚   â”‚   â”œâ”€â”€ missing_fields.txt         # Fields missing from documentation
â”‚   â”‚   â”‚   â”œâ”€â”€ README.md                  # Release-specific information
â”‚   â”‚   â”‚   â”œâ”€â”€ risk_scores.yaml           # Risk assessment scores
â”‚   â”‚   â”‚   â””â”€â”€ sbom.json                  # Software Bill of Materials
â”‚   â”‚   â””â”€â”€ e6e6d597-b9c2-48cb-804b-5a9aad99c146/  # Another run ID
â”‚   â””â”€â”€ templates/
â”‚       â”œâ”€â”€ annex_iv_template.j2 # Annex IV template
â”‚       â”œâ”€â”€ sample_inputs.json # Sample inputs for Annex IV
â”‚       â””â”€â”€ qms/ # Quality management system documentation
â”‚           â”œâ”€â”€ qms_template.md # Core QMS document
â”‚           â”œâ”€â”€ roles_and_responsibilities.md # Role assignments
â”‚           â”œâ”€â”€ audit_plan.md # Audit schedule and methodology
â”‚           â””â”€â”€ sops/ # Standard Operating Procedures
â”‚               â”œâ”€â”€ model_release_sop.md # Model release protocol
â”‚               â”œâ”€â”€ drift_monitoring_sop.md # Monitoring procedures
â”‚               â”œâ”€â”€ incident_response_sop.md # Incident handling
â”‚               â”œâ”€â”€ risk_mitigation_sop.md # Risk management process
â”‚               â””â”€â”€ data_ingestion_sop.md # Data handling procedures
â”‚
â”œâ”€â”€ assets/ # Pipeline diagrams
â”œâ”€â”€ run.py # CLI entrypoint
â””â”€â”€ README.md
```

## Running Pipelines

```bash
# Run feature engineering pipeline (Articles 10, 12)
python run.py --feature

# Run model training pipeline (Articles 9, 11, 15)
python run.py --train

# Run deployment pipeline (Articles 14, 17, 18)
python run.py --deploy
```

Options:

- `--auto-approve` for nonâ€‘interactive deployment
- `--no-cache` to disable ZenML caching
- `--config-dir <path>` to override default configs

### Configuration

Pipeline configurations are stored in the `src/configs/` directory:

- `feature_engineering.yaml`
- `training.yaml`
- `deployment.yaml`

You can specify a custom config directory using the `--config-dir` option.

## Modal Deployment

![Modal Deployment](docs/modal-deployment.png)

The project implements a serverless deployment using Modal with comprehensive monitoring and incident reporting capabilities:

- FastAPI application with documented endpoints
- Automated model and preprocessing pipeline loading
- Drift detection and incident reporting
- Standardized storage paths for compliance artifacts

## ðŸ”— EU AI Act Compliance Mapping

For a complete overview of the EU AI Act compliance mapping, refer to the [detailed pipeline steps to articles mapping and interdependencies for full compliance](COMPLIANCE.md).

## Compliance Directory Structure

| Directory               | Purpose                                                 | Auto/Manual |
| ----------------------- | ------------------------------------------------------- | ----------- |
| **records/**            | Automated compliance records from pipeline runs         | Auto        |
| **manual_fills/**       | Manual compliance inputs and preprocessing info         | Manual      |
| **monitoring/**         | Post-market monitoring records and drift detection logs | Auto        |
| **deployment_records/** | Model deployment history and model cards                | Auto        |
| **approval_records/**   | Human approval records and rationales                   | Manual      |
| **templates/**          | Jinja template for Annex IV document generation         | Manual      |

## ðŸ“‹ Quality Management System (Articleâ€¯ 17)

This repo delivers all _technical evidence_ (lineage, metadata, logs). For a complete QMS, you must also maintain formal QMS documentation.

See `compliance/qms/` for starter templates:

- **Quality Policy** (`qms_template.md`)
- **Roles & Responsibilities** (`roles_and_responsibilities.md`)
- **Audit Plan** (`audit_plan.md`)
- **SOPs** (`sops/` folder: data ingestion, model release, risk mitigation, etc.)

## Docs

For detailed explanations of each pipeline and step, refer to the [detailed pipeline documentation](docs/detailed_pipeline_explanations.md).
