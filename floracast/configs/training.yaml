# FloraCast Training Configuration
# This config is used for local development and training

settings:
  docker:
    requirements: requirements.txt
    python_package_installer: uv

model:
  name: floracast_tft
  license: apache
  description: "TFT model for DFG time series forecasting use case"
  audience: "DFG forecasting team"
  use_cases: >
    Demonstrates ZenML MLOps capabilities for time series forecasting using 
    Temporal Fusion Transformer with custom Darts materializers and proper 
    artifact management for production forecasting workflows.
  ethics: "No ethical concerns - synthetic ecommerce data for demonstration"
  tags:
    - forecasting
    - tft
    - darts
    - time-series
    - production
    - dfg

steps:
  ingest_data:
    parameters:
      data_source: "ecommerce_default"
      data_path: null
      datetime_col: "ds"
      target_col: "y"
  
  preprocess_data:
    parameters:
      datetime_col: "ds"
      target_col: "y"
      freq: "D"
      val_ratio: 0.2
  
  train_model:
    parameters:
      input_chunk_length: 14   # Longer input for better pattern recognition
      output_chunk_length: 14  # 2-week forecasting horizon for impressive demo  
      hidden_size: 16          # Smaller hidden size to prevent instability
      lstm_layers: 1           # Single layer
      num_attention_heads: 1   # Single head to prevent complexity issues
      dropout: 0.0             # No dropout to eliminate regularization issues
      batch_size: 4            # Small batch size that works with data
      n_epochs: 5              # Few epochs
      random_state: 42
      add_relative_index: true   # Required for TFT - generates future covariates
      enable_progress_bar: true
      enable_model_summary: true
      learning_rate: 0.001     # Standard learning rate that works
      weight_decay: 0.0        # No weight decay to eliminate regularization issues
  
  evaluate:
    parameters:
      horizon: 14  # Match updated output_chunk_length - 2 weeks
      metric: "smape"
